{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data** **cleaning**"
      ],
      "metadata": {
        "id": "hPbN8tXCN57E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODg2W9KNL3Oh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "\n",
        "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "file_path = os.path.join(CURRENT_DIR,\"raw\" , \"ObesityDataSet.csv\")\n",
        "\n",
        "# Charger le dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Afficher les premiÃ¨res lignes pour vÃ©rifier le chargement des donnÃ©es\n",
        "print(\"AperÃ§u des donnÃ©es :\")\n",
        "print(df.head())\n",
        "\n",
        "# VÃ©rifier les valeurs manquantes\n",
        "print(\"\n",
        "Valeurs manquantes par colonne :\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# VÃ©rifier si certaines valeurs sont codÃ©es diffÃ©remment comme manquantes\n",
        "print(\"\n",
        "Valeurs potentiellement manquantes sous d'autres formes ('?', 'None', '') :\")\n",
        "print(df.isin(['?', 'None', '']).sum())\n",
        "\n",
        "# Obtenir un rÃ©sumÃ© des donnÃ©es\n",
        "print(\"\n",
        "RÃ©sumÃ© des colonnes et valeurs non nulles :\")\n",
        "print(df.info())\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "# Affichage des outliers avec un boxplot\n",
        "numerical_cols = df.select_dtypes(include=['number']).columns\n",
        "df_numerical = df[numerical_cols]\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.boxplot(data=df_numerical)\n",
        "plt.title(\"Boxplots des variables numÃ©riques\", fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Suppression des outliers\n",
        "lower_weight, upper_weight = 40, 105\n",
        "lower_age, upper_age = 10, 26\n",
        "\n",
        "df_filtered = df[\n",
        "    (df['Weight'] >= lower_weight) & (df['Weight'] <= upper_weight) &\n",
        "    (df['Age'] >= lower_age) & (df['Age'] <= upper_age)\n",
        "]\n",
        "\n",
        "# Sauvegarde des donnÃ©es nettoyÃ©es\n",
        "processed_path = os.path.join(CURRENT_DIR, \"data\", \"processed\", \"dataset.csv\")\n",
        "\n",
        "# CrÃ©er le dossier si inexistant\n",
        "os.makedirs(os.path.dirname(processed_path), exist_ok=True)\n",
        "df_filtered.to_csv(processed_path, index=False)\n",
        "print(f'âœ… DonnÃ©es nettoyÃ©es sauvegardÃ©es dans : {processed_path}')\n",
        "\n",
        "# Encodage de la colonne cible\n",
        "le = LabelEncoder()\n",
        "df_filtered['NObeyesdad'] = le.fit_transform(df_filtered['NObeyesdad'])\n",
        "\n",
        "# Sauvegarde de l'encodeur\n",
        "encoder_path = os.path.join(CURRENT_DIR, \"data\", \"processed\", \"label_encoder.pkl\")\n",
        "with open(encoder_path, \"wb\") as file:\n",
        "    pickle.dump(le, file)\n",
        "\n",
        "print(f'âœ… Label Encoder sauvegardÃ© dans : {encoder_path}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model TRaining**"
      ],
      "metadata": {
        "id": "pPTUbkSdNehx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "import psutil\n",
        "import os\n",
        "import pickle\n",
        "import shap\n",
        "\n",
        "\n",
        "# === data loading ===\n",
        "# Get the absolute path of the current script (inside views/)\n",
        "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "# Move up one level to reach the project root\n",
        "BASE_DIR = os.path.abspath(os.path.join(CURRENT_DIR, \"..\"))\n",
        "\n",
        "# Construct paths relative to the project root\n",
        "DATA_PATH = os.path.join(BASE_DIR, \"data\",\"processed\" , \"dataset.csv\")\n",
        "df= pd.read_csv(DATA_PATH)\n",
        "\n",
        "# === memory reducing ===\n",
        "def optimize_dataframe(df):\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        df[col] = df[col].astype('int32')\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    return df\n",
        "\n",
        "df = optimize_dataframe(df)\n",
        "categorical_columns = [\"Gender\", \"family_history_with_overweight\", \"NObeyesdad\",\"FAVC\",\"SMOKE\",\"CAEC\",\"SCC\",\"CALC\",\"MTRANS\",]\n",
        "\n",
        "# ==== spliting data ====\n",
        "# Apply Label Encoding\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le  # Save encoders if you need to inverse transform later\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = df.drop(\"NObeyesdad\", axis=1)  # Features\n",
        "y = df[\"NObeyesdad\"]  # Target (Obesity Level)\n",
        "\n",
        "\n",
        "# Split into 80% Training and 20% Testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Training Set Size:\", X_train.shape)\n",
        "print(\"Testing Set Size:\", X_test.shape)\n",
        "\n",
        "\n",
        "# ==== FLAGS FOR SAMPLING METHODS ====\n",
        "USE_SMOTE = True           # Enable/Disable Oversampling\n",
        "USE_UNDERSAMPLING = True  # Enable/Disable Undersampling\n",
        "USE_CLASS_WEIGHTS = False  # Set this to True to use class weighting\n",
        "\n",
        "\n",
        "# Apply Oversampling (SMOTE)\n",
        "if USE_SMOTE:\n",
        "    smote = SMOTE(sampling_strategy=\"auto\", random_state=42)  # 60% oversampling\n",
        "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "    print(\"Applied SMOTE Oversampling. New Training Set Size:\", X_train.shape)\n",
        "\n",
        "# Apply Undersampling (RandomUnderSampler)\n",
        "if USE_UNDERSAMPLING:\n",
        "    undersample = RandomUnderSampler(sampling_strategy=\"auto\", random_state=42)  # 80% of majority class\n",
        "    X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
        "    print(\"Applied Random UnderSampling. New Training Set Size:\", X_train.shape)\n",
        "\n",
        "# Compute Class Weights (If Selected)\n",
        "class_weight_dict = None\n",
        "if USE_CLASS_WEIGHTS:\n",
        "    class_weights = compute_class_weight(class_weight=\"balanced\",\n",
        "                                         classes=np.unique(y_train), y=y_train)\n",
        "    class_weight_dict = {cls: weight for cls, weight in zip(np.unique(y_train), class_weights)}\n",
        "    print(\"Applied Class Weights:\", class_weight_dict)\n",
        "\n",
        "\n",
        "# ===== training =====\n",
        "\n",
        "# Initialize Model with Selected Class Weights\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42,\n",
        "                               class_weight=class_weight_dict if USE_CLASS_WEIGHTS else \"balanced\")\n",
        "# Train Model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Save the trained model to a file\n",
        "with open(\"obesity_model.pkl\", \"wb\") as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "\n",
        "# Train SHAP explainer after training your model\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test)  # Ensure X_test matches training format\n",
        "\n",
        "\n",
        "# Save SHAP explainer to a file\n",
        "with open(\"shap_explainer.pkl\", \"wb\") as file:\n",
        "      pickle.dump(explainer, file)\n",
        "\n",
        "print(\"SHAP explainer saved successfully as shap_explainer.pkl!\")\n",
        "\n",
        "\n",
        "# === Memmory Optimization ===\n",
        "\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / (1024 * 1024)  # Convertir en Mo\n",
        "\n",
        "memory_used = get_memory_usage()\n",
        "print(f\" Memory use after execution : {memory_used:.2f} Mo\")\n",
        "variables_a_supprimer = [var for var in globals().keys() if var not in [\"get_memory_usage\", \"gc\", \"psutil\", \"pickle\", \"shap\", \"__name__\", \"__file__\", \"__builtins__\"]]\n",
        "\n",
        "\n",
        "for var in variables_a_supprimer:\n",
        "    del globals()[var]\n",
        "\n",
        "gc.collect()\n",
        "print(\" Memory freed !\")\n"
      ],
      "metadata": {
        "id": "G4DypFLLNZPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model** **Evaluation**"
      ],
      "metadata": {
        "id": "h46pOu2JNkBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === DATA & MODEL LOADING ===\n",
        "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "BASE_DIR = os.path.abspath(os.path.join(CURRENT_DIR, \"..\"))\n",
        "\n",
        "# Construct paths\n",
        "DATA_PATH = os.path.join(BASE_DIR, \"data\", \"processed\", \"dataset.csv\")\n",
        "MODEL_PATH = os.path.join(BASE_DIR, \"models\", \"obesity_model.pkl\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Load trained model\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n",
        "\n",
        "print(f\"Loading model from: {MODEL_PATH}\")\n",
        "model = joblib.load(MODEL_PATH)\n",
        "\n",
        "# === DATA PREPARATION ===\n",
        "categorical_columns = [\"Gender\", \"family_history_with_overweight\", \"FAVC\", \"SMOKE\",\n",
        "                       \"CAEC\", \"SCC\", \"CALC\", \"MTRANS\", \"NObeyesdad\"]\n",
        "\n",
        "# Apply Label Encoding\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le  # Save encoders for inverse transform if needed\n",
        "\n",
        "# Split features & target variable\n",
        "X = df.drop(\"NObeyesdad\", axis=1)\n",
        "y = df[\"NObeyesdad\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# === PREDICTION ===\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# === MODEL EVALUATION ===\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\" Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\n Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# === CONFUSION MATRIX ===\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=set(y_test), yticklabels=set(y_test))\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# === ROC-AUC SCORE ===\n",
        "y_test_binarized = label_binarize(y_test, classes=np.unique(y_test))\n",
        "roc_auc = roc_auc_score(y_test_binarized, model.predict_proba(X_test), multi_class=\"ovr\")\n",
        "print(f\" ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# === MEMORY OPTIMIZATION ===\n",
        "def get_memory_usage():\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
        "\n",
        "memory_used = get_memory_usage()\n",
        "print(f\" Memory usage after execution: {memory_used:.2f} MB\")\n",
        "\n",
        "# Clean up memory\n",
        "variables_to_keep = {\"get_memory_usage\", \"gc\", \"psutil\", \"__name__\", \"__file__\", \"__builtins__\"}\n",
        "for var in list(globals().keys()):\n",
        "    if var not in variables_to_keep:\n",
        "        del globals()[var]\n",
        "\n",
        "gc.collect()\n",
        "print(\" Memory freed!\")\n"
      ],
      "metadata": {
        "id": "_TEr9yO9NoQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shap** **Explainer**"
      ],
      "metadata": {
        "id": "qz0FAxDoNtl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import shap\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# === DATA AND MODEL LOADING ===\n",
        "CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "BASE_DIR = os.path.abspath(os.path.join(CURRENT_DIR, \"..\"))\n",
        "DATA_PATH = os.path.join(BASE_DIR, \"data\", \"processed\", \"dataset.csv\")\n",
        "MODEL_PATH = os.path.join(BASE_DIR, \"models\", \"obesity_model.pkl\")\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n",
        "\n",
        "print(f\"ðŸ”¹ Loading model from: {MODEL_PATH}\")\n",
        "model = joblib.load(MODEL_PATH)\n",
        "\n",
        "# === DATA PREPARATION ===\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "categorical_columns = [\"Gender\", \"family_history_with_overweight\", \"FAVC\", \"SMOKE\",\n",
        "                       \"CAEC\", \"SCC\", \"CALC\", \"MTRANS\"]\n",
        "\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le  # Save encoders for inverse transform if needed\n",
        "\n",
        "X = df.drop(\"NObeyesdad\", axis=1)\n",
        "y = df[\"NObeyesdad\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# === SHAP EXPLAINER ===\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# === Ploting Shap ===\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "print(\" SHAP analysis completed successfully!\")\n"
      ],
      "metadata": {
        "id": "vbQuVIS-Nwx3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}