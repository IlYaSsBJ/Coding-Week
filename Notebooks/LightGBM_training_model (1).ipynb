{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Training model using lightGBM on both raw and modified data**"
      ],
      "metadata": {
        "id": "g3tk16Kfc4RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import lightgbm as lgb\n",
        "from lightgbm import callback\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(r\"C:\\Users\\maryam\\Pictures\\dhd_transformed_winsoring.csv\")\n",
        "\n",
        "# ------------------------\n",
        "# 1. Data Preprocessing\n",
        "# ------------------------\n",
        "# Encode categorical features (excluding target)\n",
        "for col in df.select_dtypes(include=['object']).drop(columns=[\"NObeyesdad\"]).columns:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df[\"NObeyesdad\"])\n",
        "\n",
        "# Define y_classes\n",
        "y_classes = np.unique(y)\n",
        "\n",
        "# Separate features\n",
        "X = df.drop(columns=[\"NObeyesdad\"])\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# ------------------------\n",
        "# 2. Feature Scaling\n",
        "# ------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Handling Class Imbalance\n",
        "# ------------------------\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# ------------------------\n",
        "# 4. Model Training\n",
        "# ------------------------\n",
        "print(\"\\nTraining LightGBM Classifier...\")\n",
        "lgb_train = lgb.Dataset(X_train_resampled, y_train_resampled)\n",
        "lgb_test = lgb.Dataset(X_test_scaled, y_test, reference=lgb_train)\n",
        "\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(y_classes),\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.1,\n",
        "    'num_leaves': 31,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Train the model with early stopping\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    num_boost_round=100,\n",
        "    valid_sets=[lgb_test],\n",
        "    callbacks=[callback.early_stopping(stopping_rounds=10)],\n",
        "\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 5. Model Evaluation\n",
        "# ------------------------\n",
        "# Predictions\n",
        "y_pred = np.argmax(model.predict(X_test_scaled), axis=1)\n",
        "y_pred_proba = model.predict(X_test_scaled)  # Predicted probabilities\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "print(f\"ROC-AUC Score (One-vs-Rest): {roc_auc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(\n",
        "    conf_matrix,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=label_encoder.classes_,\n",
        "    yticklabels=label_encoder.classes_,\n",
        "    annot_kws={\"size\": 12}\n",
        ")\n",
        "\n",
        "# Customize the plot\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels and adjust alignment\n",
        "plt.yticks(fontsize=12)  # Adjust y-axis font size\n",
        "plt.xlabel('Predicted Label', fontsize=14)  # Add x-axis label\n",
        "plt.ylabel('True Label', fontsize=14)  # Add y-axis label\n",
        "plt.title(\"Confusion Matrix\", fontsize=16, pad=20)  # Add title and adjust padding\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qw8O9-VQk1Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import lightgbm as lgb\n",
        "from lightgbm import callback\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(r\"C:\\Users\\maryam\\Pictures\\age+weight_done.csv\")\n",
        "\n",
        "# ------------------------\n",
        "# 1. Data Preprocessing\n",
        "# ------------------------\n",
        "# Encode categorical features (excluding target)\n",
        "for col in df.select_dtypes(include=['object']).drop(columns=[\"NObeyesdad\"]).columns:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df[\"NObeyesdad\"])\n",
        "\n",
        "# Define y_classes\n",
        "y_classes = np.unique(y)\n",
        "\n",
        "# Separate features\n",
        "X = df.drop(columns=[\"NObeyesdad\"])\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# ------------------------\n",
        "# 2. Feature Scaling\n",
        "# ------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Handling Class Imbalance\n",
        "# ------------------------\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# ------------------------\n",
        "# 4. Model Training\n",
        "# ------------------------\n",
        "print(\"\\nTraining LightGBM Classifier...\")\n",
        "lgb_train = lgb.Dataset(X_train_resampled, y_train_resampled)\n",
        "lgb_test = lgb.Dataset(X_test_scaled, y_test, reference=lgb_train)\n",
        "\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(y_classes),\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.1,\n",
        "    'num_leaves': 31,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Train the model with early stopping\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    num_boost_round=100,\n",
        "    valid_sets=[lgb_test],\n",
        "    callbacks=[callback.early_stopping(stopping_rounds=10)],\n",
        "\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 5. Model Evaluation\n",
        "# ------------------------\n",
        "# Predictions\n",
        "y_pred = np.argmax(model.predict(X_test_scaled), axis=1)\n",
        "y_pred_proba = model.predict(X_test_scaled)  # Predicted probabilities\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "print(f\"ROC-AUC Score (One-vs-Rest): {roc_auc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(\n",
        "    conf_matrix,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=label_encoder.classes_,\n",
        "    yticklabels=label_encoder.classes_,\n",
        "    annot_kws={\"size\": 12}\n",
        ")\n",
        "\n",
        "# Customize the plot\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels and adjust alignment\n",
        "plt.yticks(fontsize=12)  # Adjust y-axis font size\n",
        "plt.xlabel('Predicted Label', fontsize=14)  # Add x-axis label\n",
        "plt.ylabel('True Label', fontsize=14)  # Add y-axis label\n",
        "plt.title(\"Confusion Matrix\", fontsize=16, pad=20)  # Add title and adjust padding\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kxF4XZlIkjRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing different ways to balance the imbalanced data**"
      ],
      "metadata": {
        "id": "LPZcCZMXd4lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import lightgbm as lgb\n",
        "from lightgbm import callback\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Function to load and preprocess dataset\n",
        "def load_and_preprocess(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Encode categorical features (excluding target)\n",
        "    for col in df.select_dtypes(include=['object']).drop(columns=[\"NObeyesdad\"]).columns:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "    # Encode target variable\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(df[\"NObeyesdad\"])\n",
        "    X = df.drop(columns=[\"NObeyesdad\"])\n",
        "    return X, y, label_encoder\n",
        "\n",
        "# Function to train and evaluate LightGBM model\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, method_name):\n",
        "    # Feature Scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Train LightGBM model\n",
        "    lgb_train = lgb.Dataset(X_train_scaled, y_train)\n",
        "    lgb_test = lgb.Dataset(X_test_scaled, y_test, reference=lgb_train)\n",
        "\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': len(np.unique(y_train)),\n",
        "        'metric': 'multi_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        lgb_train,\n",
        "        num_boost_round=100,\n",
        "        valid_sets=[lgb_test],\n",
        "        callbacks=[callback.early_stopping(stopping_rounds=10)],\n",
        "\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    y_pred = np.argmax(model.predict(X_test_scaled), axis=1)\n",
        "    y_pred_proba = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "    return accuracy, roc_auc\n",
        "\n",
        "# Load datasets\n",
        "dataset1 = 'C:\\\\Users\\\\maryam\\\\Pictures\\\\age+weight_done.csv'\n",
        "dataset2 = 'C:\\\\Users\\\\maryam\\\\Pictures\\\\dhd_transformed_winsoring .csv'\n",
        "\n",
        "X1, y1, _ = load_and_preprocess(dataset1)\n",
        "X2, y2, _ = load_and_preprocess(dataset2)\n",
        "\n",
        "# Split data\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, stratify=y1, random_state=42)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, stratify=y2, random_state=42)\n",
        "\n",
        "# Initialize results table\n",
        "results = []\n",
        "\n",
        "# Undersampling\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X1_train_under, y1_train_under = undersampler.fit_resample(X1_train, y1_train)\n",
        "X2_train_under, y2_train_under = undersampler.fit_resample(X2_train, y2_train)\n",
        "\n",
        "acc1, roc1 = train_and_evaluate(X1_train_under, y1_train_under, X1_test, y1_test, \"Undersampling\")\n",
        "acc2, roc2 = train_and_evaluate(X2_train_under, y2_train_under, X2_test, y2_test, \"Undersampling\")\n",
        "results.append([\"Undersampling\", dataset1, acc1, roc1])\n",
        "results.append([\"Undersampling\", dataset2, acc2, roc2])\n",
        "\n",
        "# Oversampling (SMOTE)\n",
        "smote = SMOTE(random_state=42)\n",
        "X1_train_over, y1_train_over = smote.fit_resample(X1_train, y1_train)\n",
        "X2_train_over, y2_train_over = smote.fit_resample(X2_train, y2_train)\n",
        "\n",
        "acc1, roc1 = train_and_evaluate(X1_train_over, y1_train_over, X1_test, y1_test, \"Oversampling (SMOTE)\")\n",
        "acc2, roc2 = train_and_evaluate(X2_train_over, y2_train_over, X2_test, y2_test, \"Oversampling (SMOTE)\")\n",
        "results.append([\"Oversampling (SMOTE)\", dataset1, acc1, roc1])\n",
        "results.append([\"Oversampling (SMOTE)\", dataset2, acc2, roc2])\n",
        "\n",
        "# Class Weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y1_train), y=y1_train)\n",
        "class_weight_dict = dict(zip(np.unique(y1_train), class_weights))\n",
        "\n",
        "acc1, roc1 = train_and_evaluate(X1_train, y1_train, X1_test, y1_test, \"Class Weights\")\n",
        "acc2, roc2 = train_and_evaluate(X2_train, y2_train, X2_test, y2_test, \"Class Weights\")\n",
        "results.append([\"Class Weights\", dataset1, acc1, roc1])\n",
        "results.append([\"Class Weights\", dataset2, acc2, roc2])\n",
        "\n",
        "# Create comparison table\n",
        "comparison_table = pd.DataFrame(results, columns=[\"Method\", \"Dataset\", \"Accuracy\", \"ROC-AUC Score\"])\n",
        "print(comparison_table)"
      ],
      "metadata": {
        "id": "xq0D1w9WeL_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import lightgbm as lgb\n",
        "from lightgbm import callback\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(r\"C:\\Users\\maryam\\Pictures\\age+weight_done.csv\")\n",
        "\n",
        "# ------------------------\n",
        "# 1. Data Preprocessing\n",
        "# ------------------------\n",
        "# Encode categorical features (excluding target)\n",
        "for col in df.select_dtypes(include=['object']).drop(columns=[\"NObeyesdad\"]).columns:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df[\"NObeyesdad\"])\n",
        "\n",
        "# Define y_classes\n",
        "y_classes = np.unique(y)\n",
        "\n",
        "# Separate features\n",
        "X = df.drop(columns=[\"NObeyesdad\"])\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# ------------------------\n",
        "# 2. Feature Scaling\n",
        "# ------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Handling Class Imbalance\n",
        "# ------------------------\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# ------------------------\n",
        "# 4. Model Training\n",
        "# ------------------------\n",
        "print(\"\\nTraining LightGBM Classifier...\")\n",
        "lgb_train = lgb.Dataset(X_train_resampled, y_train_resampled)\n",
        "lgb_test = lgb.Dataset(X_test_scaled, y_test, reference=lgb_train)\n",
        "\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(y_classes),\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.1,\n",
        "    'num_leaves': 31,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Train the model with early stopping\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    num_boost_round=100,\n",
        "    valid_sets=[lgb_test],\n",
        "    callbacks=[callback.early_stopping(stopping_rounds=10)],\n",
        "\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 5. Model Evaluation\n",
        "# ------------------------\n",
        "# Predictions\n",
        "y_pred = np.argmax(model.predict(X_test_scaled), axis=1)\n",
        "y_pred_proba = model.predict(X_test_scaled)  # Predicted probabilities\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "print(f\"ROC-AUC Score (One-vs-Rest): {roc_auc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(\n",
        "    conf_matrix,\n",
        "    annot=True,\n",
        "    fmt=\"d\",\n",
        "    cmap=\"Blues\",\n",
        "    xticklabels=label_encoder.classes_,\n",
        "    yticklabels=label_encoder.classes_,\n",
        "    annot_kws={\"size\": 12}\n",
        ")\n",
        "\n",
        "# Customize the plot\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12)  # Rotate x-axis labels and adjust alignment\n",
        "plt.yticks(fontsize=12)  # Adjust y-axis font size\n",
        "plt.xlabel('Predicted Label', fontsize=14)  # Add x-axis label\n",
        "plt.ylabel('True Label', fontsize=14)  # Add y-axis label\n",
        "plt.title(\"Confusion Matrix\", fontsize=16, pad=20)  # Add title and adjust padding\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zz8aUicYkqLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Working with the best option offering best accurracy (the modified data + OVERSAMPLING)**"
      ],
      "metadata": {
        "id": "P9t1AOxQePUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import lightgbm as lgb\n",
        "from lightgbm import callback\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('C:\\\\Users\\\\maryam\\\\Pictures\\\\age+weight_done.csv')\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 1. Data Preprocessing\n",
        "# ------------------------\n",
        "# Encode categorical features (excluding target)\n",
        "for col in df.select_dtypes(include=['object']).drop(columns=[\"NObeyesdad\"]).columns:\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df[\"NObeyesdad\"])\n",
        "\n",
        "# Separate features\n",
        "X = df.drop(columns=[\"NObeyesdad\"])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# ------------------------\n",
        "# 2. Feature Scaling\n",
        "# ------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Oversampling with SMOTE\n",
        "# ------------------------\n",
        "print(\"\\nApplying SMOTE for oversampling...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# ------------------------\n",
        "# 4. Train LightGBM Model\n",
        "# ------------------------\n",
        "print(\"\\nTraining LightGBM Classifier...\")\n",
        "lgb_train = lgb.Dataset(X_train_resampled, y_train_resampled)\n",
        "lgb_test = lgb.Dataset(X_test_scaled, y_test, reference=lgb_train)\n",
        "\n",
        "# Define LightGBM parameters\n",
        "params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(np.unique(y_train_resampled)),\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.1,\n",
        "    'num_leaves': 31,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    num_boost_round=100,\n",
        "    valid_sets=[lgb_test],\n",
        "    callbacks=[callback.early_stopping(stopping_rounds=10)],\n",
        "\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 5. SHAP Summary Plot\n",
        "# ------------------------\n",
        "# Initialize SHAP explainer\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test_scaled)\n",
        "\n",
        "# Create SHAP summary plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns, class_names=label_encoder.classes_, show=False)\n",
        "plt.title(\"SHAP Summary Plot: Feature Importance\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yzvPxWsBejMs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}